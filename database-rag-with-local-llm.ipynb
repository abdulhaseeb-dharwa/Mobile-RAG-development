{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c55b3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29e9811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "os.environ[\"LLAMA_CPP_LOG_LEVEL\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "837cd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TableInfo:\n",
    "    name: str\n",
    "    columns: List[Dict[str, Any]]\n",
    "    relationships: List[Dict[str, str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b47ac666",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca26cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseSchemaManager:\n",
    "    \"\"\"\n",
    "    Manages database schema operations and caching.\n",
    "    \n",
    "    Attributes:\n",
    "        db_path (str): Path to the SQLite database file\n",
    "        connection (Optional[sqlite3.Connection]): Database connection object\n",
    "        schema_cache (Optional[Dict]): Cached schema information\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str):\n",
    "        self.db_path = db_path\n",
    "        self.connection: Optional[sqlite3.Connection] = None\n",
    "        self.schema_cache: Optional[Dict] = None\n",
    "        self._lock = threading.Lock()  # Thread safety for schema operations\n",
    "\n",
    "    @contextmanager\n",
    "    def get_connection(self):\n",
    "        \"\"\"Context manager for database connections.\"\"\"\n",
    "        try:\n",
    "            if not self.connection:\n",
    "                self.connect()\n",
    "            yield self.connection\n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Database connection error: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            if self.connection:\n",
    "                self.connection.close()\n",
    "                self.connection = None\n",
    "\n",
    "    def connect(self) -> None:\n",
    "        \"\"\"Establishes a connection to the database.\"\"\"\n",
    "        try:\n",
    "            self.connection = sqlite3.connect(self.db_path)\n",
    "            self.connection.row_factory = sqlite3.Row  # Enable row factory for better access\n",
    "            logger.info(f\"Connected to database at {self.db_path}\")\n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Error connecting to database: {e}\")\n",
    "            raise\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Closes the database connection.\"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            self.connection = None\n",
    "            logger.info(\"Database connection closed\")\n",
    "\n",
    "    def get_schema(self, refresh: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Retrieves the database schema with caching.\n",
    "        \n",
    "        Args:\n",
    "            refresh (bool): Force refresh of schema cache\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Database schema information\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            if self.schema_cache is not None and not refresh:\n",
    "                return self.schema_cache\n",
    "\n",
    "            if not self.connection:\n",
    "                self.connect()\n",
    "\n",
    "            schema = {\"tables\": [], \"relationships\": []}\n",
    "            \n",
    "            try:\n",
    "                with self.get_connection() as conn:\n",
    "                    cursor = conn.cursor()\n",
    "                    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\")\n",
    "                    tables = cursor.fetchall()\n",
    "\n",
    "                    for table in tables:\n",
    "                        table_name = table[0]\n",
    "                        table_info = {\"name\": table_name, \"columns\": []}\n",
    "                        \n",
    "                        # Get column information\n",
    "                        cursor.execute(f\"PRAGMA table_info('{table_name}');\")\n",
    "                        columns = cursor.fetchall()\n",
    "                        for col in columns:\n",
    "                            table_info[\"columns\"].append({\n",
    "                                \"name\": col[\"name\"],\n",
    "                                \"type\": col[\"type\"],\n",
    "                                \"is_primary_key\": bool(col[\"pk\"]),\n",
    "                                \"not_null\": bool(col[\"notnull\"]),\n",
    "                                \"default\": col[\"dflt_value\"]\n",
    "                            })\n",
    "                        schema[\"tables\"].append(table_info)\n",
    "\n",
    "                        # Get foreign key information\n",
    "                        cursor.execute(f\"PRAGMA foreign_key_list('{table_name}');\")\n",
    "                        foreign_keys = cursor.fetchall()\n",
    "                        for fk in foreign_keys:\n",
    "                            schema[\"relationships\"].append({\n",
    "                                \"table\": table_name,\n",
    "                                \"column\": fk[\"from\"],\n",
    "                                \"references_table\": fk[\"table\"],\n",
    "                                \"references_column\": fk[\"to\"]\n",
    "                            })\n",
    "\n",
    "                self.schema_cache = schema\n",
    "                return schema\n",
    "                \n",
    "            except sqlite3.Error as e:\n",
    "                logger.error(f\"Error retrieving schema: {e}\")\n",
    "                raise\n",
    "\n",
    "    def format_schema_for_llm(self) -> str:\n",
    "        \"\"\"\n",
    "        Formats the database schema for LLM consumption.\n",
    "        \n",
    "        Returns:\n",
    "            str: Formatted schema string\n",
    "        \"\"\"\n",
    "        schema = self.get_schema()\n",
    "        formatted = \"### DATABASE SCHEMA\\n\\n\"\n",
    "        \n",
    "        for table in schema[\"tables\"]:\n",
    "            column_defs = []\n",
    "            for col in table[\"columns\"]:\n",
    "                flags = []\n",
    "                if col[\"is_primary_key\"]:\n",
    "                    flags.append(\"PK\")\n",
    "                if col[\"not_null\"]:\n",
    "                    flags.append(\"NN\")\n",
    "                flag_str = \" \".join(flags)\n",
    "                column_defs.append(f\"{col['name']} {col['type']} {flag_str}\".strip())\n",
    "            formatted += f\"{table['name']}({', '.join(column_defs)});\\n\"\n",
    "            \n",
    "        return formatted\n",
    "\n",
    "    def filter_relevant_tables(self, user_query: str) -> str:\n",
    "        \"\"\"\n",
    "        Filters and returns schema information for tables relevant to the query.\n",
    "        \n",
    "        Args:\n",
    "            user_query (str): User's natural language query\n",
    "            \n",
    "        Returns:\n",
    "            str: Filtered schema information\n",
    "        \"\"\"\n",
    "        schema = self.get_schema()\n",
    "        query_lower = user_query.lower()\n",
    "\n",
    "        # Step 1: Match tables directly mentioned in the query\n",
    "        relevant_tables = set()\n",
    "        for table in schema[\"tables\"]:\n",
    "            if table[\"name\"].lower() in query_lower:\n",
    "                relevant_tables.add(table[\"name\"])\n",
    "\n",
    "        # Step 2: Fallback to frequently used tables if no matches found\n",
    "        if not relevant_tables:\n",
    "            fallback_tables = [\"customer\", \"countries\", \"prospect\", \"visit\"]\n",
    "            relevant_tables.update([t for t in fallback_tables if t in [tbl[\"name\"] for tbl in schema[\"tables\"]]])\n",
    "\n",
    "        # Step 3: Build Compact Schema for relevant tables only\n",
    "        formatted = \"### DATABASE SCHEMA\\n\\n\"\n",
    "        for table in schema[\"tables\"]:\n",
    "            if table[\"name\"] in relevant_tables:\n",
    "                column_defs = []\n",
    "                for col in table[\"columns\"]:\n",
    "                    flags = []\n",
    "                    if col[\"is_primary_key\"]:\n",
    "                        flags.append(\"PK\")\n",
    "                    if col[\"not_null\"]:\n",
    "                        flags.append(\"NN\")\n",
    "                    flag_str = \" \".join(flags)\n",
    "                    column_defs.append(f\"{col['name']} {col['type']} {flag_str}\".strip())\n",
    "                formatted += f\"{table['name']}({', '.join(column_defs)});\\n\"\n",
    "                \n",
    "        return formatted\n",
    "\n",
    "    def get_sample_data(self, limit: int = 3) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Retrieves sample data from all tables.\n",
    "        \n",
    "        Args:\n",
    "            limit (int): Number of rows to retrieve per table\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, pd.DataFrame]: Dictionary of table names to sample data\n",
    "        \"\"\"\n",
    "        schema = self.get_schema()\n",
    "        samples = {}\n",
    "        \n",
    "        try:\n",
    "            with self.get_connection() as conn:\n",
    "                for table in schema[\"tables\"]:\n",
    "                    try:\n",
    "                        query = f\"SELECT * FROM {table['name']} LIMIT {limit};\"\n",
    "                        samples[table['name']] = pd.read_sql_query(query, conn)\n",
    "                    except sqlite3.Error as e:\n",
    "                        logger.warning(f\"Error retrieving sample data for table {table['name']}: {e}\")\n",
    "                        samples[table['name']] = pd.DataFrame()\n",
    "                        \n",
    "            return samples\n",
    "            \n",
    "        except sqlite3.Error as e:\n",
    "            logger.error(f\"Error retrieving sample data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def format_sample_data_for_llm(self, limit: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Formats sample data for LLM consumption.\n",
    "        \n",
    "        Args:\n",
    "            limit (int): Number of rows to retrieve per table\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted sample data\n",
    "        \"\"\"\n",
    "        samples = self.get_sample_data(limit)\n",
    "        formatted = \"SAMPLE DATA:\\n\\n\"\n",
    "        \n",
    "        for table, df in samples.items():\n",
    "            formatted += f\"Table: {table}\\n\"\n",
    "            if df.empty:\n",
    "                formatted += \"  (No data available)\\n\\n\"\n",
    "            else:\n",
    "                table_str = df.to_string(index=False)\n",
    "                formatted += \"\\n\".join(\"  \" + line for line in table_str.split(\"\\n\")) + \"\\n\\n\"\n",
    "                \n",
    "        return formatted\n",
    "\n",
    "\n",
    "    def get_sample_data(self, limit: int = 3):\n",
    "        if not self.connection:\n",
    "            self.connect()\n",
    "        schema = self.get_schema()\n",
    "        samples = {}\n",
    "        for table in schema[\"tables\"]:\n",
    "            try:\n",
    "                query = f\"SELECT * FROM {table['name']} LIMIT {limit};\"\n",
    "                samples[table['name']] = pd.read_sql_query(query, self.connection)\n",
    "            except sqlite3.Error:\n",
    "                samples[table['name']] = pd.DataFrame()\n",
    "        return samples\n",
    "\n",
    "    def format_sample_data_for_llm(self, limit: int = 3) -> str:\n",
    "        samples = self.get_sample_data(limit)\n",
    "        formatted = \"SAMPLE DATA:\\n\\n\"\n",
    "        for table, df in samples.items():\n",
    "            formatted += f\"Table: {table}\\n\"\n",
    "            if df.empty:\n",
    "                formatted += \"  (No data available)\\n\\n\"\n",
    "            else:\n",
    "                table_str = df.to_string(index=False)\n",
    "                formatted += \"\\n\".join(\"  \" + line for line in table_str.split(\"\\n\")) + \"\\n\\n\"\n",
    "        return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ab4c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalLLMClient:\n",
    "    def __init__(self, model_path: str):\n",
    "        self.llm = None\n",
    "        self.is_loaded = False\n",
    "        self.load_thread = threading.Thread(target=self._load_model, args=(model_path,), daemon=True)\n",
    "        self.load_thread.start()\n",
    "\n",
    "    def _load_model(self, model_path):\n",
    "        try:\n",
    "            self.llm = Llama(model_path=model_path, n_ctx=16384, n_gpu_layers=0)\n",
    "            self.is_loaded = True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model load error: {e}\")\n",
    "\n",
    "    def wait_for_model(self, timeout=30):\n",
    "        self.load_thread.join(timeout)\n",
    "        return self.is_loaded\n",
    "\n",
    "    def generate_sql(self, prompt: str) -> str:\n",
    "        if not self.is_loaded and not self.wait_for_model():\n",
    "            return \"Model not ready.\"\n",
    "\n",
    "        # Wrap prompt with instruction template\n",
    "        wrapped_prompt = f\"\"\"You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n",
    "    ### Instruction:\n",
    "    {prompt}\n",
    "    ### Response:\"\"\"\n",
    "\n",
    "        # Tokenize and truncate if needed\n",
    "        encoded = self.llm.tokenize(wrapped_prompt.encode(\"utf-8\"))\n",
    "        print(f\"Encoded prompt: {len(encoded)}\")\n",
    "        max_context_tokens = 16384\n",
    "        max_response_tokens = 1024\n",
    "        max_prompt_tokens = max_context_tokens - max_response_tokens\n",
    "\n",
    "        if len(encoded) > max_prompt_tokens:\n",
    "            logger.warning(f\"Prompt too long ({len(encoded)} tokens), truncating to {max_prompt_tokens} tokens.\")\n",
    "            encoded = encoded[:max_prompt_tokens]\n",
    "            wrapped_prompt = self.llm.detokenize(encoded).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        try:\n",
    "            output = self.llm(wrapped_prompt, max_tokens=max_response_tokens, stop=[\"### Instruction:\", \"### Response:\"], echo=False)\n",
    "            return output[\"choices\"][0][\"text\"].strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"LLM error: {e}\")\n",
    "            return \"Error generating SQL.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2f5e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryProcessor:\n",
    "    def __init__(self, llm_client, schema_manager):\n",
    "        self.llm_client = llm_client\n",
    "        self.schema_manager = schema_manager\n",
    "\n",
    "    def process_query(self, user_query: str):\n",
    "        # Get schema chunks\n",
    "        schema_info = self.schema_manager.filter_relevant_tables(user_query)\n",
    "        # sample_data = self.schema_manager.format_sample_data_for_llm(limit=1)\n",
    "        \n",
    "        prompt = self._create_sql_generation_prompt(user_query, schema_info)\n",
    "        # logger.info(f\"Prompt size estimate: {len(prompt.split())} tokens\")\n",
    "        # n_tokens = len(self.llm_client.llm.tokenize(prompt.encode('utf-8')))\n",
    "        # print(f\"Prompt size : {n_tokens} tokens\")\n",
    "        print(schema_info)\n",
    "        \n",
    "        response = self.llm_client.generate_sql(prompt)\n",
    "        return {\"sql\": self._parse_sql(response)}\n",
    "\n",
    "    def _create_sql_generation_prompt(self, user_query: str, schema_info: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "    ### USER QUESTION START\n",
    "    {user_query}\n",
    "    ### USER QUESTION END\n",
    "\n",
    "    {schema_info}\n",
    "\n",
    "    ### INSTRUCTIONS\n",
    "    Provide only the SQL query inside triple backticks (```). Don't include anything else in your response.\n",
    "    Strictly use the table names provided in the schema.\n",
    "    If the required tables are not in this schema, respond with \"TABLES_NOT_IN_CHUNK\".\n",
    "        \"\"\"\n",
    "        return prompt.strip()\n",
    "\n",
    "    def _parse_sql(self, response: str) -> str:\n",
    "        match = re.search(r\"```sql\\s*(.*?)\\s*```\", response, re.DOTALL)\n",
    "        return match.group(1).strip() if match else response.strip()\n",
    "\n",
    "    def validate_sql(self, sql: str):\n",
    "        if not self.schema_manager.connection:\n",
    "            self.schema_manager.connect()\n",
    "        try:\n",
    "            self.schema_manager.connection.execute(f\"EXPLAIN QUERY PLAN {sql}\")\n",
    "            return True, \"\"\n",
    "        except sqlite3.Error as e:\n",
    "            return False, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f6e822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryExecutor:\n",
    "    def __init__(self, schema_manager):\n",
    "        self.schema_manager = schema_manager\n",
    "\n",
    "    def execute_query(self, sql: str):\n",
    "        if not self.schema_manager.connection:\n",
    "            self.schema_manager.connect()\n",
    "        try:\n",
    "            df = pd.read_sql_query(sql, self.schema_manager.connection)\n",
    "            return {\"status\": \"success\", \"results\": df, \"row_count\": len(df)}\n",
    "        except sqlite3.Error as e:\n",
    "            return {\"status\": \"error\", \"error_message\": str(e), \"results\": None}\n",
    "\n",
    "    def format_results(self, execution_result: Dict) -> Dict:\n",
    "        if execution_result[\"status\"] == \"error\":\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Query execution failed: {execution_result['error_message']}\",\n",
    "                \"data\": None,\n",
    "                \"row_count\": 0,\n",
    "                \"columns\": [],\n",
    "                \"summary\": {}\n",
    "            }\n",
    "\n",
    "        df = execution_result[\"results\"]\n",
    "        records = df.to_dict(orient=\"records\")\n",
    "        summary = {\n",
    "            col: {\n",
    "                \"min\": float(df[col].min()),\n",
    "                \"max\": float(df[col].max()),\n",
    "                \"mean\": float(df[col].mean()),\n",
    "                \"median\": float(df[col].median())\n",
    "            } for col in df.select_dtypes(include=np.number).columns\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": f\"Query returned {len(records)} rows\",\n",
    "            \"data\": records,\n",
    "            \"row_count\": len(records),\n",
    "            \"columns\": list(df.columns),\n",
    "            \"summary\": summary\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "329987dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineRAGAgent:\n",
    "    def __init__(self, db_path: str, model_path: str):\n",
    "        self.schema_manager = DatabaseSchemaManager(db_path)\n",
    "        self.llm_client = LocalLLMClient(model_path)\n",
    "        self.query_processor = QueryProcessor(self.llm_client, self.schema_manager)\n",
    "        self.query_executor = QueryExecutor(self.schema_manager)\n",
    "        self.schema_manager.connect()\n",
    "        self.schema_manager.get_schema()\n",
    "\n",
    "    def process_query(self, user_query: str) -> Dict:\n",
    "        if not self.llm_client.is_loaded and not self.llm_client.wait_for_model(timeout=5):\n",
    "            return {\"status\": \"pending\", \"message\": \"Model loading...\", \"user_query\": user_query}\n",
    "\n",
    "        query_result = self.query_processor.process_query(user_query)\n",
    "        sql = query_result[\"sql\"]\n",
    "        print(f\"SQL: {sql}\")  \n",
    "        is_valid, error = self.query_processor.validate_sql(sql)\n",
    "\n",
    "        if not is_valid:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Invalid SQL: {error}\",\n",
    "                \"user_query\": user_query,\n",
    "                \"sql\": sql,\n",
    "                \"results\": None\n",
    "            }\n",
    "\n",
    "        execution_result = self.query_executor.execute_query(sql)\n",
    "        formatted_results = self.query_executor.format_results(execution_result)\n",
    "\n",
    "        return {\n",
    "            \"status\": formatted_results[\"status\"],\n",
    "            \"message\": formatted_results[\"message\"],\n",
    "            \"user_query\": user_query,\n",
    "            \"sql\": sql,\n",
    "            \"results\": formatted_results[\"data\"],\n",
    "            \"columns\": formatted_results.get(\"columns\", []),\n",
    "            \"summary\": formatted_results.get(\"summary\", {})\n",
    "        }\n",
    "\n",
    "    def close(self):\n",
    "        self.schema_manager.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18e03cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 14:29:36,054 - INFO - Connected to database at cargill.db\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 219 tensors from deepseek-coder-1.3b-instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = deepseek-ai_deepseek-coder-1.3b-instruct\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5504\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 100000.000000\n",
      "llama_model_loader: - kv  11:                    llama.rope.scale_linear f32              = 4.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32256]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32256]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,31757]   = [\"ƒ† ƒ†\", \"ƒ† t\", \"ƒ† a\", \"i n\", \"h e...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 32013\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32021\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32014\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type q5_0:   12 tensors\n",
      "llama_model_loader: - type q8_0:   12 tensors\n",
      "llama_model_loader: - type q4_K:  133 tensors\n",
      "llama_model_loader: - type q6_K:   13 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 831.88 MiB (5.18 BPW) \n",
      "load: missing pre-tokenizer type, using: 'default'\n",
      "load:                                             \n",
      "load: ************************************        \n",
      "load: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "load: CONSIDER REGENERATING THE MODEL             \n",
      "load: ************************************        \n",
      "load:                                             \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control-looking token:  32015 '<ÔΩúfim‚ñÅholeÔΩú>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: control-looking token:  32017 '<ÔΩúfim‚ñÅendÔΩú>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: control-looking token:  32016 '<ÔΩúfim‚ñÅbeginÔΩú>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: control token:  32014 '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>' is not marked as EOG\n",
      "load: control token:  32015 '<ÔΩúfim‚ñÅholeÔΩú>' is not marked as EOG\n",
      "load: control token:  32017 '<ÔΩúfim‚ñÅendÔΩú>' is not marked as EOG\n",
      "load: control token:  32016 '<ÔΩúfim‚ñÅbeginÔΩú>' is not marked as EOG\n",
      "load: control token:  32013 '<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>' is not marked as EOG\n",
      "load: control token:  32021 '<|EOT|>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special_eot_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 240\n",
      "load: token to piece cache size = 0.1792 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 16384\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 24\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 16\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 2048\n",
      "print_info: n_embd_v_gqa     = 2048\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 5504\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 100000.0\n",
      "print_info: freq_scale_train = 0.25\n",
      "print_info: n_ctx_orig_yarn  = 16384\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 1.35 B\n",
      "print_info: general.name     = deepseek-ai_deepseek-coder-1.3b-instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 32256\n",
      "print_info: n_merges         = 31757\n",
      "print_info: BOS token        = 32013 '<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>'\n",
      "print_info: EOS token        = 32021 '<|EOT|>'\n",
      "print_info: EOT token        = 32014 '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'\n",
      "print_info: PAD token        = 32014 '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'\n",
      "print_info: LF token         = 185 'ƒä'\n",
      "print_info: FIM PRE token    = 32016 '<ÔΩúfim‚ñÅbeginÔΩú>'\n",
      "print_info: FIM SUF token    = 32015 '<ÔΩúfim‚ñÅholeÔΩú>'\n",
      "print_info: FIM MID token    = 32017 '<ÔΩúfim‚ñÅendÔΩú>'\n",
      "print_info: EOG token        = 32014 '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'\n",
      "print_info: EOG token        = 32021 '<|EOT|>'\n",
      "print_info: max token length = 128\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 86 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Model loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_tensors:  CPU_AARCH64 model buffer size =   479.25 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   831.88 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      "...............................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 16384\n",
      "llama_context: n_ctx_per_seq = 16384\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 100000.0\n",
      "llama_context: freq_scale    = 0.25\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 16384 (padded)\n",
      "llama_kv_cache_unified: kv_size = 16384, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =  3072.00 MiB\n",
      "llama_kv_cache_unified: KV self size  = 3072.00 MiB, K (f16): 1536.00 MiB, V (f16): 1536.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   560.01 MiB\n",
      "llama_context: graph nodes  = 822\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'deepseek-ai_deepseek-coder-1.3b-instruct', 'general.architecture': 'llama', 'llama.context_length': '16384', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '2048', 'llama.block_count': '24', 'llama.feed_forward_length': '5504', 'llama.attention.head_count': '16', 'tokenizer.ggml.eos_token_id': '32021', 'general.file_type': '15', 'llama.attention.head_count_kv': '16', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.freq_base': '100000.000000', 'llama.rope.scale_linear': '4.000000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '32013', 'tokenizer.ggml.padding_token_id': '32014'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    db_path = \"cargill.db\"\n",
    "    model_path = \"deepseek-coder-1.3b-instruct.Q4_K_M.gguf\"\n",
    "    agent = OfflineRAGAgent(db_path, model_path)\n",
    "    result = agent.process_query(\"Find the total number of customers as total_customers in the United States\")\n",
    "    if result[\"status\"] == \"success\":\n",
    "        print(\"‚úÖ Generated SQL:\\n\", result[\"sql\"])\n",
    "        print(\"üìä SQL Results:\\n\", pd.DataFrame(result[\"results\"]))\n",
    "    elif result[\"status\"] == \"pending\":\n",
    "        print(\"‚è≥\", result[\"message\"])\n",
    "    else:\n",
    "        print(\"‚ùå Error:\", result[\"message\"])\n",
    "    agent.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e3cdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
