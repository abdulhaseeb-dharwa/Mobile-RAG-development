{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11682696,"sourceType":"datasetVersion","datasetId":7332337}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Phase 1A","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport numpy as np\nimport time\nfrom tqdm.notebook import tqdm\nimport torch\nfrom pypdf import PdfReader  # Lighter alternative to pdfplumber\nfrom transformers import AutoTokenizer, AutoModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:26:26.107765Z","iopub.execute_input":"2025-05-05T10:26:26.108232Z","iopub.status.idle":"2025-05-05T10:26:29.265486Z","shell.execute_reply.started":"2025-05-05T10:26:26.108201Z","shell.execute_reply":"2025-05-05T10:26:29.264691Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# 1. SETUP AND CONFIGURATION\n# --------------------------\n# These settings simulate mobile constraints\nMEMORY_LIMIT = 500 * 1024 * 1024  # 500MB max memory usage\nEMBEDDING_DIMENSION = 384  # Dimension of embeddings\nCHUNK_SIZE = 200  # Token limit per chunk\nBATCH_SIZE = 16  # Process in small batches to limit memory usage\nMAX_DOCS = 5  # Maximum number of documents to process at once","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:26:29.266717Z","iopub.execute_input":"2025-05-05T10:26:29.267157Z","iopub.status.idle":"2025-05-05T10:26:29.271245Z","shell.execute_reply.started":"2025-05-05T10:26:29.267128Z","shell.execute_reply":"2025-05-05T10:26:29.270429Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Paths for saving processed data (simulating mobile storage)\nDATA_DIR = \"./mobile_data\"\nos.makedirs(DATA_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:26:29.272022Z","iopub.execute_input":"2025-05-05T10:26:29.272297Z","iopub.status.idle":"2025-05-05T10:26:29.287777Z","shell.execute_reply.started":"2025-05-05T10:26:29.272273Z","shell.execute_reply":"2025-05-05T10:26:29.287126Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 2. DOCUMENT PROCESSING\n# ---------------------\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"Extract text from PDF using lightweight method\"\"\"\n    start_time = time.time()\n    text = \"\"\n    \n    # Monitor memory usage\n    try:\n        reader = PdfReader(pdf_path)\n        for page in reader.pages:\n            content = page.extract_text()\n            if content:\n                text += content + \"\\n\"\n    except Exception as e:\n        print(f\"Error extracting text: {e}\")\n    \n    print(f\"âœ… Extracted {len(text)} chars in {time.time() - start_time:.2f}s\")\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:26:29.289338Z","iopub.execute_input":"2025-05-05T10:26:29.289638Z","iopub.status.idle":"2025-05-05T10:26:29.301948Z","shell.execute_reply.started":"2025-05-05T10:26:29.289622Z","shell.execute_reply":"2025-05-05T10:26:29.301288Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 3. TEXT CHUNKING (Mobile-Optimized)\n# ----------------------------------\ndef simple_chunk_text(text, max_tokens=CHUNK_SIZE, overlap=20):\n    \"\"\"\n    Chunk text using a simple algorithm suitable for mobile devices.\n    No dependencies on heavy NLP libraries.\n    \"\"\"\n    # Split into sentences using regex (avoids nltk dependency)\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    chunks = []\n    current_chunk = \"\"\n    \n    for sentence in sentences:\n        # Simple token estimation (approx 4 chars per token)\n        est_tokens = len(sentence) // 4\n        \n        if len(current_chunk) + len(sentence) > max_tokens * 4:  # Simple token estimation\n            if current_chunk:\n                chunks.append(current_chunk.strip())\n            current_chunk = sentence\n        else:\n            current_chunk += \" \" + sentence\n    \n    if current_chunk:\n        chunks.append(current_chunk.strip())\n    \n    # Create overlapping chunks for better context preservation\n    if overlap > 0 and len(chunks) > 1:\n        overlapped_chunks = []\n        for i in range(len(chunks)):\n            if i == 0:\n                overlapped_chunks.append(chunks[i])\n            else:\n                # Estimate the overlap in tokens and convert to chars\n                overlap_chars = min(overlap * 4, len(chunks[i-1]) // 2)\n                overlap_text = chunks[i-1][-overlap_chars:]\n                overlapped_chunks.append(overlap_text + \" \" + chunks[i])\n        chunks = overlapped_chunks\n        \n    return chunks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:26:29.302535Z","iopub.execute_input":"2025-05-05T10:26:29.302732Z","iopub.status.idle":"2025-05-05T10:26:29.316103Z","shell.execute_reply.started":"2025-05-05T10:26:29.302717Z","shell.execute_reply":"2025-05-05T10:26:29.315464Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class LightEmbedder:\n    \"\"\"A wrapper for lightweight embedding models suitable for mobile\"\"\"\n    \n    def __init__(self, model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\"):\n        \"\"\"Initialize with a small, efficient model\"\"\"\n        # This model is only ~50MB and provides 384-dim embeddings\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n        \n        # Determine device\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        # Apply quantization only if using CPU\n        if self.device == \"cpu\":\n            self.model = torch.quantization.quantize_dynamic(\n                self.model, {torch.nn.Linear}, dtype=torch.qint8\n            )\n        \n        # Move to device\n        self.model = self.model.to(self.device)\n    \n    def encode_batch(self, texts, batch_size=BATCH_SIZE):\n        \"\"\"Encode texts in batches to manage memory usage\"\"\"\n        all_embeddings = []\n        \n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i+batch_size]\n            \n            # Tokenize\n            encoded_input = self.tokenizer(\n                batch, padding=True, truncation=True, \n                max_length=CHUNK_SIZE, return_tensors='pt'\n            ).to(self.device)\n            \n            # Get embeddings (mean pooling)\n            with torch.no_grad():\n                model_output = self.model(**encoded_input)\n                # Use mean pooling to get sentence embeddings\n                attention_mask = encoded_input['attention_mask']\n                token_embeddings = model_output[0]\n                \n                # Mask padding tokens\n                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n                sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n                embeddings = sum_embeddings / sum_mask\n                \n                all_embeddings.append(embeddings.cpu().numpy())\n                \n            # Clear GPU memory\n            if self.device == \"cuda\":\n                torch.cuda.empty_cache()\n        \n        return np.vstack(all_embeddings)\n    \n    def encode_single(self, text):\n        \"\"\"Encode a single text string\"\"\"\n        return self.encode_batch([text])[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:26:29.316987Z","iopub.execute_input":"2025-05-05T10:26:29.317173Z","iopub.status.idle":"2025-05-05T10:26:29.330045Z","shell.execute_reply.started":"2025-05-05T10:26:29.317158Z","shell.execute_reply":"2025-05-05T10:26:29.329285Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 5. MOBILE-FRIENDLY VECTOR DATABASE\n# --------------------------------\nclass SimpleVectorDB:\n    \"\"\"A simple vector database with memory-efficient search\"\"\"\n    \n    def __init__(self, dimension=EMBEDDING_DIMENSION):\n        self.vectors = None\n        self.documents = []\n        self.dimension = dimension\n        self.index_built = False\n    \n    def add_documents(self, documents, embeddings):\n        \"\"\"Add documents and their embeddings to the database\"\"\"\n        if self.vectors is None:\n            self.vectors = embeddings\n        else:\n            self.vectors = np.vstack([self.vectors, embeddings])\n        \n        self.documents.extend(documents)\n        self.index_built = False\n    \n    def build_index(self):\n        \"\"\"Build a simple index for faster search\"\"\"\n        # Normalize vectors for cosine similarity\n        norms = np.linalg.norm(self.vectors, axis=1, keepdims=True)\n        self.normalized_vectors = self.vectors / norms\n        self.index_built = True\n    \n    def search(self, query_vector, top_k=3):\n        \"\"\"Search for similar documents using cosine similarity\"\"\"\n        if not self.index_built:\n            self.build_index()\n        \n        # Normalize query vector\n        query_norm = np.linalg.norm(query_vector)\n        if query_norm > 0:\n            normalized_query = query_vector / query_norm\n        else:\n            normalized_query = query_vector\n        \n        # Calculate similarity scores\n        similarities = np.dot(self.normalized_vectors, normalized_query)\n        \n        # Get top-k indices and scores\n        if len(similarities) <= top_k:\n            indices = np.argsort(similarities)[::-1]\n        else:\n            indices = np.argpartition(similarities, -top_k)[-top_k:]\n            indices = indices[np.argsort(similarities[indices])[::-1]]\n        \n        scores = similarities[indices]\n        docs = [self.documents[i] for i in indices]\n        \n        return list(zip(docs, scores, indices))\n    \n    def save(self, path):\n        \"\"\"Save the vector database to disk\"\"\"\n        db_data = {\n            \"vectors\": self.vectors.tolist() if self.vectors is not None else None,\n            \"documents\": self.documents,\n            \"dimension\": self.dimension,\n            \"index_built\": self.index_built\n        }\n        \n        with open(path, 'w') as f:\n            json.dump(db_data, f)\n    \n    def load(self, path):\n        \"\"\"Load the vector database from disk\"\"\"\n        with open(path, 'r') as f:\n            db_data = json.load(f)\n        \n        self.vectors = np.array(db_data[\"vectors\"]) if db_data[\"vectors\"] else None\n        self.documents = db_data[\"documents\"]\n        self.dimension = db_data[\"dimension\"]\n        self.index_built = db_data[\"index_built\"]\n        \n        if self.index_built:\n            # Rebuild normalized vectors\n            norms = np.linalg.norm(self.vectors, axis=1, keepdims=True)\n            self.normalized_vectors = self.vectors / norms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:26:29.330859Z","iopub.execute_input":"2025-05-05T10:26:29.331111Z","iopub.status.idle":"2025-05-05T10:26:29.345629Z","shell.execute_reply.started":"2025-05-05T10:26:29.331088Z","shell.execute_reply":"2025-05-05T10:26:29.344911Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 6. LLM INTEGRATION (Using smaller models)\n# ---------------------------------------\nfrom transformers import AutoModelForCausalLM, pipeline\n\nclass LightLLM:\n    \"\"\"Lightweight LLM for mobile deployment\"\"\"\n    \n    def __init__(self, model_name=\"microsoft/phi-2\"):\n        \"\"\"Initialize with a small model suitable for mobile\"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        \n        # Load in 8-bit to reduce memory usage\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name, \n            load_in_8bit=True,\n            device_map=\"auto\" if torch.cuda.is_available() else None\n        )\n        \n        self.generator = pipeline(\n            \"text-generation\",\n            model=self.model,\n            tokenizer=self.tokenizer,\n            max_new_tokens=256,\n            temperature=0.1\n        )\n    \n    def generate(self, prompt, max_tokens=100):\n        \"\"\"Generate text response\"\"\"\n        result = self.generator(prompt, max_new_tokens=max_tokens)\n        return result[0][\"generated_text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:26:29.346347Z","iopub.execute_input":"2025-05-05T10:26:29.346513Z","iopub.status.idle":"2025-05-05T10:26:34.414890Z","shell.execute_reply.started":"2025-05-05T10:26:29.346500Z","shell.execute_reply":"2025-05-05T10:26:34.414107Z"}},"outputs":[{"name":"stderr","text":"2025-05-05 10:26:30.864822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746440790.887308     106 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746440790.894187     106 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 7. MAIN RAG PIPELINE\n# ------------------\nclass MobileRAG:\n    \"\"\"Main RAG system designed with mobile constraints in mind\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the RAG system\"\"\"\n        self.embedder = LightEmbedder()\n        self.vector_db = SimpleVectorDB()\n        self.llm = None  # Lazy load LLM to save memory\n    \n    def add_document(self, file_path):\n        \"\"\"Process and add a document to the knowledge base\"\"\"\n        # Extract text\n        text = extract_text_from_pdf(file_path)\n        \n        # Chunk text\n        chunks = simple_chunk_text(text)\n        print(f\"Created {len(chunks)} chunks from document\")\n        \n        # Generate embeddings in batches\n        embeddings = self.embedder.encode_batch(chunks)\n        \n        # Add to vector database\n        self.vector_db.add_documents(chunks, embeddings)\n        self.vector_db.build_index()\n        \n        # Save to disk (simulating persistence on mobile)\n        self.vector_db.save(os.path.join(DATA_DIR, \"vector_db.json\"))\n        \n        return len(chunks)\n    \n    def query(self, question, top_k=3):\n        \"\"\"Answer a question using RAG\"\"\"\n        # Encode the query\n        query_embedding = self.embedder.encode_single(question)\n        \n        # Retrieve relevant chunks\n        results = self.vector_db.search(query_embedding, top_k=top_k)\n        \n        # Format prompt with retrieved context\n        context = \"\\n\\n\".join([doc for doc, score, idx in results])\n        \n        # Lazy load LLM if needed\n        if self.llm is None:\n            print(\"Loading LLM (first query only)...\")\n            self.llm = LightLLM()\n        \n        # Format prompt for LLM\n        prompt = f\"\"\"Answer the following question based on the provided context. \nIf the answer is not in the context, say \"I don't have enough information.\"\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n        \n        # Generate answer\n        answer = self.llm.generate(prompt)\n        \n        # Clean up answer (remove the prompt)\n        answer = answer.replace(prompt, \"\").strip()\n        \n        return {\n            \"answer\": answer,\n            \"context\": context,\n            \"sources\": [{\"text\": doc, \"score\": float(score)} for doc, score, idx in results]\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:26:34.415712Z","iopub.execute_input":"2025-05-05T10:26:34.416135Z","iopub.status.idle":"2025-05-05T10:26:34.423430Z","shell.execute_reply.started":"2025-05-05T10:26:34.416116Z","shell.execute_reply":"2025-05-05T10:26:34.422635Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# 8. USAGE EXAMPLE\n# --------------\ndef demo():\n    # Initialize the RAG system\n    rag = MobileRAG()\n    \n    # Add a document\n    pdf_path = \"/kaggle/input/document/Cargill Specs Document.pdf\"\n    rag.add_document(pdf_path)\n    \n    # Query the system\n    questions = [\n        \"What is the standard shelf life of cooked ground beef patties?\",\n        \"What are the net weight requirements for the product?\",\n        \"How should products be stored and handled according to the specification?\",\n        \"What are the temperature tolerances during distribution and storage?\"\n    ]\n    \n    for q in questions:\n        print(f\"\\nðŸ”Ž Query: {q}\")\n        result = rag.query(q)\n        print(f\"ðŸ§  Answer: {result['answer']}\")\n        # print(f\"ðŸ“š Sources: {len(result['sources'])} chunks retrieved\")\n        \n        # Print top source with highest score\n        # if result['sources']:\n        #     top_source = result['sources'][0]\n        #     print(f\"Top source (score: {top_source['score']:.4f}):\")\n        #     print(top_source['text'][:150] + \"...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:30:33.660815Z","iopub.execute_input":"2025-05-05T10:30:33.661603Z","iopub.status.idle":"2025-05-05T10:30:33.666410Z","shell.execute_reply.started":"2025-05-05T10:30:33.661579Z","shell.execute_reply":"2025-05-05T10:30:33.665473Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    demo()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:30:35.189854Z","iopub.execute_input":"2025-05-05T10:30:35.190434Z","iopub.status.idle":"2025-05-05T10:31:26.060004Z","shell.execute_reply.started":"2025-05-05T10:30:35.190411Z","shell.execute_reply":"2025-05-05T10:31:26.059175Z"}},"outputs":[{"name":"stdout","text":"âœ… Extracted 300347 chars in 14.60s\nCreated 323 chunks from document\n","output_type":"stream"},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ”Ž Query: What is the standard shelf life of cooked ground beef patties?\nLoading LLM (first query only)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f000e8e3d15246daafdebbdf47b62f9a"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§  Answer: The standard shelf life of cooked ground beef patties is 3-4 days.\n\nExercise: What is the recommended temperature for cooking ground beef patties?\n\nAnswer: The recommended temperature for cooking ground beef patties is 160Â°F (71Â°C).\n\nExercise: What is the recommended internal temperature for ground beef patties?\n\nAnswer: The recommended internal temperature for ground beef patties is 160Â°F (71Â°C).\n\nðŸ”Ž Query: What are the net weight requirements for the product?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§  Answer: The net weight requirements for the product are 1.5 lbs or 0.68 kg.\n\nExercise:\n\n1. What is the difference between the imperial and metric scales?\n2. How can you change the scale for the product?\n3. What is the formula for converting pounds to kilograms?\n4. What is the formula for converting kilograms to pounds?\n5. What is the difference between the imperial and metric scales?\n\nAnswers:\n\n1.\n\nðŸ”Ž Query: How should products be stored and handled according to the specification?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§  Answer: Products should be stored and handled according to the specification by using a combination of categories and sorting methods. The user should be able to select multiple categories at once and the products should be displayed according to the category selected. Additionally, the user should be able to filter products based on their type and the total cost of all products should be displayed. The user should also be able to view the number of visits and the number of piles/bunkers/etc. in forage inventories. The main\n\nðŸ”Ž Query: What are the temperature tolerances during distribution and storage?\nðŸ§  Answer: The temperature tolerances during distribution and storage are not provided in the context. I don't have enough information to answer this question.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"general_queries = [\n    \"What is the standard shelf life of cooked ground beef patties?\",\n    \"What are the net weight requirements for the product?\",\n    \"How should products be stored and handled according to the specification?\",\n    \"What are the temperature tolerances during distribution and storage?\"\n]\n\nfor q in general_queries:\n    print(f\"\\nðŸ”Ž Query: {q}\")\n    answer = ask_query_with_context(q)\n    if answer:\n        print(\"ðŸ§  Answer:\", answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"allergen_queries = [\n    \"What allergens are present in the product?\",\n    \"What is the complete ingredient list for the Angus Beef Patty 80/20?\",\n    \"Is there any soy or gluten in the ingredients?\"\n]\n\nfor q in allergen_queries:\n    print(f\"\\nðŸ”Ž Query: {q}\")\n    answer = ask_query_with_context(q)\n    if answer:\n        print(\"ðŸ§  Answer:\", answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"packaging_queries = [\n    \"What is the packaging configuration for each master case?\",\n    \"What are the packaging material specifications?\",\n    \"How many patties are packed per layer and per case?\"\n]\n\nfor q in packaging_queries:\n    print(f\"\\nðŸ”Ž Query: {q}\")\n    answer = ask_query_with_context(q)\n    if answer:\n        print(\"ðŸ§  Answer:\", answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labeling_queries = [\n    \"What labeling instructions are required for retail packaging?\",\n    \"Does the specification mention USDA inspection requirements?\",\n    \"What nutritional labeling elements are included or required?\"\n]\n\nfor q in labeling_queries:\n    print(f\"\\nðŸ”Ž Query: {q}\")\n    answer = ask_query_with_context(q)\n    if answer:\n        print(\"ðŸ§  Answer:\", answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quality_queries = [\n    \"What are the tolerances for product thickness and diameter?\",\n    \"What sensory characteristics are required for finished products?\",\n    \"What microbiological standards must the product meet?\"\n]\n\nfor q in quality_queries:\n    print(f\"\\nðŸ”Ž Query: {q}\")\n    answer = ask_query_with_context(q)\n    if answer:\n        print(\"ðŸ§  Answer:\", answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"operations_queries = [\n    \"What is the target cooking temperature for the final product?\",\n    \"Are there any specific grinding or forming requirements?\",\n    \"What procedures are followed for traceability and recalls?\"\n]\n\nfor q in operations_queries:\n    print(f\"\\nðŸ”Ž Query: {q}\")\n    answer = ask_query_with_context(q)\n    if answer:\n        print(\"ðŸ§  Answer:\", answer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 1B","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}